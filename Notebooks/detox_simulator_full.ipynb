{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe899fe-2038-410b-b018-dd8be6c15a8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Detox usecase Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "788c7fde-8240-4fb2-8c30-f3ad9b9a6f7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.5\"  # Change this to your version if needed\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "import pydeequ\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "from pyspark.sql import SparkSession\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edbddfd0-b2ce-4af6-a71a-5451e2721433",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Randomization functions\n",
    "The following functions implements a dataset with random properties for simulation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f10c07-8aed-4da0-88af-5313e284f360",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_data_table(spark):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame containing randomly generated data.\n",
    "\n",
    "    The DataFrame will have the following columns:\n",
    "    - 'Scores': Random integers between 0 and 10.\n",
    "    - 'Temperature': Random integers between -10 and 50.\n",
    "    - 'RandomValues': Random floating-point numbers between 0 and 1, with some values randomly set to None (NaN).\n",
    "    - 'NormallyDistributed': Random values generated from a normal distribution with a mean of 10 and a standard deviation of 4.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A PySpark DataFrame containing the generated data with four columns.\n",
    "    \"\"\"\n",
    "    num_rows = random.choice([95, 200, 300])\n",
    "    scores = [random.randint(0, 10) for _ in range(num_rows)]\n",
    "    temperature = [random.randint(-10, 50) for _ in range(num_rows)]\n",
    "    random_values = [random.uniform(0, 1) for _ in range(num_rows)]\n",
    "    normally_distributed = [random.gauss(0, 1) for _ in range(num_rows)]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        'Scores': scores,\n",
    "        'Temperature': temperature,\n",
    "        'RandomValues': random_values,\n",
    "        'NormallyDistributed': normally_distributed\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = spark.createDataFrame([(s, t, r, n) for s, t, r, n in zip(scores, temperature, random_values, normally_distributed)],\n",
    "                                schema=['Scores', 'Temperature', 'RandomValues', 'NormallyDistributed'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def bump_df(df, spark):\n",
    "    \"\"\"\n",
    "    Randomly modifies a Spark DataFrame by altering values in specific columns.\n",
    "\n",
    "    This function performs the following operations on the input DataFrame `df` with a \n",
    "    probability of 20%:\n",
    "    \n",
    "    1. Randomly replaces values in the 'Scores' column with random values between -10 \n",
    "       and 10. This replacement occurs with a probability of 50%.\n",
    "       \n",
    "    2. Randomly inserts NaN (None) values into the 'RandomValues' column with a \n",
    "       probability of 30%.\n",
    "       \n",
    "    3. Randomly replaces values in the 'Temperature' column with -999 with a \n",
    "       probability of 40%.\n",
    "\n",
    "    Parameters:\n",
    "    df (pyspark.sql.DataFrame): The input Spark DataFrame to be modified.\n",
    "    spark (pyspark.sql.SparkSession): The Spark session object (not used in the current implementation).\n",
    "\n",
    "    Returns:\n",
    "    pyspark.sql.DataFrame: The modified Spark DataFrame with potential changes in the \n",
    "                           'Scores', 'RandomValues', and 'Temperature' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, when, rand, expr\n",
    "    import numpy as np\n",
    "\n",
    "    # Insert random values greater or lower than 10 into random positions in the 'Scores' column\n",
    "    df = df.withColumn(\n",
    "        'Scores',\n",
    "        when(rand() < 0.10, (rand() * 20) - 10).otherwise(col('Scores'))\n",
    "    )\n",
    "\n",
    "    # Insert NaNs into random positions in the 'RandomValues' column\n",
    "    df = df.withColumn(\n",
    "        'RandomValues',\n",
    "        when(rand() < 0.10, None).otherwise(col('RandomValues'))\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        'NormallyDistributed',\n",
    "        when(rand() < 0.10, (rand()*10)).otherwise(col('NormallyDistributed'))\n",
    "    )\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_data_table(spark)\n",
    "bdf = bump_df(df, spark)\n",
    "bdf.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6a9b13-ca65-4cff-b9d5-82e99dd0d77d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Pydeequ step\n",
    "In this second phase, thwe implement the Pydeequ checks over some of the columns of the matrices and yields .json logs about their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715d0a4d-c211-4d40-b622-8725df1bab11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pydeequ_check(df, spark):\n",
    "    \"\"\"\n",
    "        Performs data quality checks on a Spark DataFrame using PyDeequ.\n",
    "\n",
    "        This function defines a series of data quality checks to validate the contents \n",
    "        of the input DataFrame `df`. The checks include verifying the size of the DataFrame, \n",
    "        ensuring that the 'Scores' column meets specified minimum and maximum values, \n",
    "        checking for completeness in the 'RandomValues' column, and validating the \n",
    "        non-negativity of 'Scores'. Additionally, it assesses the mean and standard \n",
    "        deviation of a normally distributed column.\n",
    "\n",
    "        The checks are executed with a warning level, and the results are returned in JSON format.\n",
    "\n",
    "        Parameters:\n",
    "        df (pyspark.sql.DataFrame): The input Spark DataFrame to be validated.\n",
    "        spark (pyspark.sql.SparkSession): The Spark session object used for executing checks.\n",
    "\n",
    "        Returns:\n",
    "        dict: A JSON representation of the verification results, including the outcomes of the defined data quality checks.\n",
    "    \"\"\"\n",
    "    # Define checks using PyDeequ\n",
    "    check = Check(spark, CheckLevel.Warning, \"Data Quality Checks\")\n",
    "\n",
    "    # Add checks for the DataFrame\n",
    "    check_result = (\n",
    "            check\n",
    "            .hasSize(lambda x: x == 300)  # Check for DataFrame size\n",
    "            .hasMin(\"Scores\", lambda x: x == 0.0)  # Scores should have a minimum of 0\n",
    "            .hasMax(\"Scores\", lambda x: x == 10.0)  # Scores should have a maximum of 10\n",
    "            .isComplete(\"RandomValues\")  # Check for completeness in RandomValues\n",
    "            .isNonNegative(\"Scores\")  # Check Temperature for non-negativity\n",
    "            .hasMean(\"NormallyDistributed\", lambda x: x > 0.0)  # Check Normally Distributed values\n",
    "            .hasStandardDeviation(\"NormallyDistributed\", lambda x: x >  1.0)\n",
    "    )\n",
    "    \n",
    "    result = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "    \n",
    "    #return VerificationResult.checkResultsAsDataFrame(spark, result, pandas=True)\n",
    "    return VerificationResult.checkResultsAsJson(spark, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dceb052c-c4fc-47fa-99c6-c8f1a4cfc22f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Cloudwatch step\n",
    "\n",
    "Here, the handler for logging the results from the last step, the Pydequ data quality logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3cf4f06-1dfb-4690-86ed-ee5bc99fbb03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_to_cloudwatch(json_result, client, log_group_name='your-log-group', log_stream_name='your-log-stream'):\n",
    " \n",
    "    # Create log group if it doesn't exist\n",
    "    try:\n",
    "        client.create_log_group(logGroupName=log_group_name)\n",
    "    except client.exceptions.ResourceAlreadyExistsException:\n",
    "        pass\n",
    "\n",
    "    # Create log stream if it doesn't exist\n",
    "    try:\n",
    "        client.create_log_stream(logGroupName=log_group_name, logStreamName=log_stream_name)\n",
    "    except client.exceptions.ResourceAlreadyExistsException:\n",
    "        pass\n",
    "\n",
    "    # Prepare log events from the JSON result\n",
    "    log_events = []\n",
    "    for log in json_result:\n",
    "        log_entry = {\n",
    "            'check_status': log['check_status'],\n",
    "            'check_level': log['check_level'],\n",
    "            'constraint_status': log['constraint_status'],\n",
    "            'check': log['check'],\n",
    "            'constraint_message': log['constraint_message'],\n",
    "            'constraint': log['constraint']\n",
    "        }\n",
    "        log_events.append({\n",
    "            'timestamp': int(time.time() * 1000),  # Current timestamp in milliseconds\n",
    "            'message': json.dumps(log_entry)  # Convert log entry to string\n",
    "        })\n",
    "\n",
    "    # Send logs to CloudWatch\n",
    "    sequence_token = None\n",
    "    while log_events:\n",
    "        chunk = log_events[:100]  # Limit to 100 log events per request\n",
    "        del log_events[:100]\n",
    "\n",
    "        # Get sequence token\n",
    "        response = client.describe_log_streams(logGroupName=log_group_name, logStreamNamePrefix=log_stream_name)\n",
    "        sequence_token = response['logStreams'][0].get('uploadSequenceToken')\n",
    "\n",
    "        # Put log events\n",
    "        if sequence_token:\n",
    "            client.put_log_events(\n",
    "                logGroupName=log_group_name,\n",
    "                logStreamName=log_stream_name,\n",
    "                logEvents=chunk,\n",
    "                sequenceToken=sequence_token\n",
    "            )\n",
    "        else:\n",
    "            client.put_log_events(\n",
    "                logGroupName=log_group_name,\n",
    "                logStreamName=log_stream_name,\n",
    "                logEvents=chunk\n",
    "            )\n",
    "\n",
    "        time.sleep(1)  # Avoid hitting rate limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f90633a9-a16e-4e98-a179-922555b12e70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Simulation main\n",
    "This cell implements:\n",
    "1. Setup the pyspark environment + spark session\n",
    "2. Create a boto3 session and logs into AWS\n",
    "3. Starts a loop across 100 steps generating random dataframes, applying pydeequ quality analysis and logs it into Cloudwatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b30bf169-d649-415a-a20c-3596d0c446b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "# Main function. Make sure you have a credential file accessible for boto3\n",
    "# This main function makes 10 trials for pydeequ + cloudwatch\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # Initialize Spark session with Deequ package\n",
    "    spark = (SparkSession\n",
    "        .builder\n",
    "        .config(\"spark.jars.excludes\", pydeequ.deequ_maven_coord)\n",
    "        .config(\"spark.jars.packages\", pydeequ.f2j_maven_coord)\n",
    "        .getOrCreate())\n",
    "\n",
    "    # Create a session with your access keys\n",
    "    #os.environ['AWS_SHARED_CREDENTIALS_FILE'] = '/Workspace/Users/jesse.americogomesdelima@gilead.com/detox/credentials'\n",
    "    session = boto3.Session(region_name='us-east-1')\n",
    "    #session = boto3.Session(session = 'hackathon')\n",
    "    \n",
    "    # Create a CloudWatch Logs client\n",
    "    client = session.client('logs')\n",
    "\n",
    "    # Set log group and log stream for our experiment\n",
    "    log_group = 'hackathon'\n",
    "    log_stream = 'detox_usecase'\n",
    "    \n",
    "    # Create 100 simulations and log results to CloudWatch\n",
    "    for i in range(1,10000):\n",
    "        df = create_data_table(spark)\n",
    "        bdf = bump_df(df, spark)\n",
    "        json_result = pydeequ_check(bdf, spark)\n",
    "        log_to_cloudwatch(json_result, client, log_group, log_stream)\n",
    "\n",
    "        # Waits between 1 and 5 seconds until next simulation\n",
    "        delay =random.randint(1,180)\n",
    "        print(f\"Simulation {i}, next step = {delay}s\")\n",
    "        time.sleep(delay)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "detox_simulator_full",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
