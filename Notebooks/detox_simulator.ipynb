{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffe899fe-2038-410b-b018-dd8be6c15a8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Detox usecase Notebook\n",
    "\n",
    "This notebook is divided in two sections. The first section creates an user defined size Dataframe with columns in order to emulate a generic DE/DS/ML pipeline result. The second session calls rules from pyDeequ to generate logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c94aef9-3eaf-4686-bc37-0e961c471098",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.3\"  # Change this to your version if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918baac5-a5ef-41fa-9df6-50c2498f7a04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydeequ in /usr/local/lib/python3.9/dist-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.9/dist-packages (from pydeequ) (2.0.2)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from pydeequ) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23.0->pydeequ) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23.0->pydeequ) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23.0->pydeequ) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23.0->pydeequ) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1056773b-c0e1-4ddd-a645-ca58a1eb9689;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;2.0.3-spark-3.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      ":: resolution report :: resolve 684ms :: artifacts dl 36ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;2.0.3-spark-3.3 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.1 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   2   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1056773b-c0e1-4ddd-a645-ca58a1eb9689\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/14ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/23 23:50:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/23 23:50:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "import pydeequ\n",
    "\n",
    "# Initialize Spark session with Deequ package\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Data Quality Checks Example\")\n",
    "    .config(\"spark.jars.packages\", \"com.amazon.deequ:deequ:2.0.1-spark-3.2\")  # Specify Deequ version\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+\n",
      "|             Scores|         Temperature|        RandomValues| NormallyDistributed|\n",
      "+-------------------+--------------------+--------------------+--------------------+\n",
      "|                1.0|  0.9520685788946797|  0.9520685788946797| 0.37753894771242275|\n",
      "|  4.186511062244069| 0.41224462431392883| 0.41224462431392883| -1.2359142700919765|\n",
      "|  7.933151473106406|              -999.0|  0.9839312708387886|-0.16365980985727172|\n",
      "|               10.0|                null|                null|  -1.225107123424144|\n",
      "|               10.0|                null|                null|  -0.131061343635309|\n",
      "|  3.283370012164891|  0.9190952257084841|  0.9190952257084841|-0.04047789955820...|\n",
      "|               10.0|              -999.0|                null|  0.6523931848660555|\n",
      "|   9.97876857428571|                null|                null| -1.8501500456629973|\n",
      "| -1.515260283681803| 0.19511099669962084| 0.19511099669962084| -0.9806762022942105|\n",
      "|                3.0|                null|                null| -0.9633252675417764|\n",
      "|                8.0| 0.48078086313726687| 0.48078086313726687|-0.13713128044194686|\n",
      "|  7.009134423746644|                null|                null| 0.21966767199319923|\n",
      "|                6.0|0.017232311307730952|0.017232311307730952|   1.421688837383299|\n",
      "|                8.0|  0.5490259212916057|  0.5490259212916057| 0.16164454329901942|\n",
      "| 1.1805468508322505|                null|                null|  0.7494468405223834|\n",
      "|  1.807461274522126| 0.15890670762115588| 0.15890670762115588|-0.11786911130008956|\n",
      "|  8.033951821175087|              -999.0|  0.4484279559060854| -1.1770896966688638|\n",
      "|                7.0|  0.4712658904758844|  0.4712658904758844|  0.8414730440254524|\n",
      "|-1.2297843165915605| 0.29284400379641007| 0.29284400379641007| -1.7129493891703806|\n",
      "|  8.037546796708934|              -999.0|  0.3186957126671637|  0.0409201850635362|\n",
      "+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "import time\n",
    "def create_data_table(spark):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame containing randomly generated data.\n",
    "\n",
    "    The DataFrame will have the following columns:\n",
    "    - 'Scores': Random integers between 0 and 10.\n",
    "    - 'Temperature': Random integers between -10 and 50.\n",
    "    - 'RandomValues': Random floating-point numbers between 0 and 1, with some values randomly set to None (NaN).\n",
    "    - 'NormallyDistributed': Random values generated from a normal distribution with a mean of 10 and a standard deviation of 4.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A PySpark DataFrame containing the generated data with four columns.\n",
    "    \"\"\"\n",
    "    num_rows = random.randint(0, 1000)\n",
    "    scores = [random.randint(0, 10) for _ in range(num_rows)]\n",
    "    temperature = [random.randint(-10, 50) for _ in range(num_rows)]\n",
    "    random_values = [random.uniform(0, 1) for _ in range(num_rows)]\n",
    "    normally_distributed = [random.gauss(0, 1) for _ in range(num_rows)]\n",
    "\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        'Scores': scores,\n",
    "        'Temperature': temperature,\n",
    "        'RandomValues': random_values,\n",
    "        'NormallyDistributed': normally_distributed\n",
    "    }\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = spark.createDataFrame([(s, t, r, n) for s, t, r, n in zip(scores, temperature, random_values, normally_distributed)],\n",
    "                                schema=['Scores', 'Temperature', 'RandomValues', 'NormallyDistributed'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def bump_df(df, spark):\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, when, rand, expr\n",
    "    import numpy as np\n",
    "\n",
    "    if np.random.random() < 0.20:\n",
    " \n",
    "        # Insert random values greater or lower than 10 into random positions in the 'Scores' column\n",
    "        df = df.withColumn(\n",
    "            'Scores',\n",
    "            when(rand() < 0.50, (rand() * 20) - 10).otherwise(col('Scores'))\n",
    "        )\n",
    "\n",
    "        # Insert NaNs into random positions in the 'RandomValues' column\n",
    "        df = df.withColumn(\n",
    "            'RandomValues',\n",
    "            when(rand() < 0.30, None).otherwise(col('RandomValues'))\n",
    "        )\n",
    "        \n",
    "        # Insert NaNs into random positions in the 'RandomValues' column\n",
    "        df = df.withColumn(\n",
    "            'Temperature',\n",
    "            when(rand() < 0.40, -999).otherwise(col('RandomValues'))\n",
    "        )\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = create_data_table(spark)\n",
    "bdf = bump_df(df, spark)\n",
    "bdf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd6a9b13-ca65-4cff-b9d5-82e99dd0d77d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Second step\n",
    "\n",
    "In this second phase, the pydeequ implements some checks in the columns of the matrices and yields logs about their results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pydeequ_check(df, spark):\n",
    "\n",
    "    from pydeequ.checks import Check, CheckLevel\n",
    "    from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Define checks using PyDeequ\n",
    "    check = Check(spark, CheckLevel.Error, \"Data Quality Checks\")\n",
    "\n",
    "    # Add checks for the DataFrame\n",
    "    check_result = (\n",
    "            check\n",
    "            .hasSize(lambda x: x == 300)  # Check for DataFrame size\n",
    "            .hasMin(\"Scores\", lambda x: x == 0.0)  # Scores should have a minimum of 0\n",
    "            .hasMax(\"Scores\", lambda x: x == 10.0)  # Scores should have a maximum of 10\n",
    "            .isComplete(\"RandomValues\")  # Check for completeness in RandomValues\n",
    "            .isNonNegative(\"Scores\")  # Check Temperature for non-negativity\n",
    "            .hasMean(\"NormallyDistributed\", lambda x: x > 0.0)  # Check Normally Distributed values\n",
    "            .hasStandardDeviation(\"NormallyDistributed\", lambda x: x >  1.0)\n",
    "\n",
    "    )\n",
    "    \n",
    "    result = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "    \n",
    "    return VerificationResult.checkResultsAsDataFrame(spark, result, pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dceb052c-c4fc-47fa-99c6-c8f1a4cfc22f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Third step\n",
    "\n",
    "Finally, Detox log each result into the Cloudwatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1\n",
      "Python Callback server started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 check check_level check_status  \\\n",
      "0  Data Quality Checks       Error        Error   \n",
      "1  Data Quality Checks       Error        Error   \n",
      "2  Data Quality Checks       Error        Error   \n",
      "3  Data Quality Checks       Error        Error   \n",
      "4  Data Quality Checks       Error        Error   \n",
      "5  Data Quality Checks       Error        Error   \n",
      "6  Data Quality Checks       Error        Error   \n",
      "\n",
      "                                          constraint constraint_status  \\\n",
      "0                         SizeConstraint(Size(None))           Failure   \n",
      "1            MinimumConstraint(Minimum(Scores,None))           Success   \n",
      "2            MaximumConstraint(Maximum(Scores,None))           Success   \n",
      "3  CompletenessConstraint(Completeness(RandomValu...           Success   \n",
      "4  ComplianceConstraint(Compliance(Scores is non-...           Success   \n",
      "5     MeanConstraint(Mean(NormallyDistributed,None))           Failure   \n",
      "6  StandardDeviationConstraint(StandardDeviation(...           Failure   \n",
      "\n",
      "                                  constraint_message  \n",
      "0  Value: 54 does not meet the constraint require...  \n",
      "1                                                     \n",
      "2                                                     \n",
      "3                                                     \n",
      "4                                                     \n",
      "5  Value: -0.1159812775793409 does not meet the c...  \n",
      "6  Value: 0.9622266754306426 does not meet the co...  \n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "def create_watchtower_log(log_group_name, log_stream_name, log_message, client):\n",
    "    \"\"\"\n",
    "    Creates a log group and log stream if they do not exist, and writes a log event with status.\n",
    "\n",
    "    Parameters:\n",
    "    - log_group_name: Name of the CloudWatch log group.\n",
    "    - log_stream_name: Name of the CloudWatch log stream.\n",
    "    - log_message: Message to log.\n",
    "    - log_status: Status to log (e.g., 'INFO', 'ERROR').\n",
    "    - client: Boto3 CloudWatch Logs client.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the log group if it doesn't exist\n",
    "    try:\n",
    "        client.create_log_group(logGroupName=log_group_name)\n",
    "    except client.exceptions.ResourceAlreadyExistsException:\n",
    "        pass\n",
    "\n",
    "    # Create the log stream if it doesn't exist\n",
    "    try:\n",
    "        client.create_log_stream(logGroupName=log_group_name, logStreamName=log_stream_name)\n",
    "    except client.exceptions.ResourceAlreadyExistsException:\n",
    "        pass\n",
    "\n",
    "    # Get the current timestamp\n",
    "    timestamp = int(time.time() * 1000)\n",
    "\n",
    "    # Prepare the log event with status\n",
    "    log_event = {\n",
    "        'timestamp': timestamp,\n",
    "        'message': str(log_message)\n",
    "\n",
    "    }\n",
    "\n",
    "    # Format the log event as JSON\n",
    "    json_log_event = json.dumps(log_event)\n",
    "\n",
    "    # Get the sequence token for the log stream\n",
    "    response = client.describe_log_streams(\n",
    "        logGroupName=log_group_name,\n",
    "        logStreamNamePrefix=log_stream_name\n",
    "    )\n",
    "    \n",
    "    sequence_token = response['logStreams'][0].get('uploadSequenceToken')\n",
    "\n",
    "    # Put the log event to CloudWatch\n",
    "    if sequence_token:\n",
    "        client.put_log_events(\n",
    "            logGroupName=log_group_name,\n",
    "            logStreamName=log_stream_name,\n",
    "            logEvents=[{\n",
    "                'timestamp': timestamp,\n",
    "                'message': json_log_event\n",
    "            }],\n",
    "            sequenceToken=sequence_token\n",
    "        )\n",
    "    else:\n",
    "        client.put_log_events(\n",
    "            logGroupName=log_group_name,\n",
    "            logStreamName=log_stream_name,\n",
    "            logEvents=[{\n",
    "                'timestamp': timestamp,\n",
    "                'message': json_log_event\n",
    "            }]\n",
    "        )\n",
    "    \n",
    "# Main function. Make sure you have a credential file accessible for boto3\n",
    "# This main function makes 10 trials for pydeequ + cloudwatch\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a session with your access keys\n",
    "    os.environ['AWS_SHARED_CREDENTIALS_FILE'] = '/opt/workspace/.aws/credentials'\n",
    "    session = boto3.Session(profile_name='hackathon', region_name='us-west-2')\n",
    "    \n",
    "    # Create a CloudWatch Logs client\n",
    "    client = session.client('logs')\n",
    "    log_group = 'hackathon'\n",
    "    log_stream = 'detox_usecase'\n",
    "    \n",
    "    \n",
    "    for i in range(1,100):\n",
    "        print(f\"step {i}\")\n",
    "        df = create_data_table(spark)\n",
    "        bdf = bump_df(df, spark)\n",
    "        checkResult_df = pydeequ_check(df, spark)\n",
    "        print(checkResult_df)\n",
    "        checkResult_df.apply(lambda row: create_watchtower_log(log_group, log_stream, row['constraint_message'], client), axis=1)\n",
    "        # Wait for 3 minutes (180 seconds)\n",
    "        time.sleep(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkResult_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Detox_usecase",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
