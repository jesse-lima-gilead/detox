{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing data quality at scale with PyDeequ\n",
    "\n",
    "Authors: Vitalina Komashko (komashk@), Calvin Wang (calviwan@), Chris Ghyzel (cghyzel@), Joan Aoanan (jaoanan@), Veronika Megler (meglerv@) \n",
    "\n",
    "__Updated June 2024 to use a new dataset, added additional library usage examples.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook accompanies AWS Blog post [Testing data quality at scale with PyDeequ](https://aws.amazon.com/blogs/big-data/testing-data-quality-at-scale-with-pydeequ/).\n",
    "\n",
    "You generally write unit tests for your code, but do you also test your data? Incoming data quality can make or break your application. Incorrect, missing, or malformed data can have a large impact on production systems. Examples of data quality issues include the following:\n",
    "\n",
    "- Missing values can lead to failures in the production system that require non-null values (`NullPointerException`)\n",
    "- Changes in the distribution of data can lead to unexpected outputs of machine learning (ML) models\n",
    "- Aggregations of incorrect data can lead to misguided business decisions\n",
    "\n",
    "In this post, we introduce PyDeequ, an open source Python wrapper over [Deequ](https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/) (an open source tool developed and used at Amazon). Deequ is written in [Scala](https://www.scala-lang.org/), whereas PyDeequ allows you to use its data quality and testing capabilities from Python and PySpark, the language of choice for many data scientists. PyDeequ democratizes and extends the power of Deequ by allowing you to use it alongside the many data science libraries that are available in that language. Furthermore, PyDeequ allows for fluid interface with [pandas](https://pandas.pydata.org/) DataFrames as opposed to restricting within [Apache Spark](https://spark.apache.org/) DataFrames.\n",
    "\n",
    "Deequ allows you to calculate data quality metrics for your dataset, define and verify data quality constraints, and be informed about changes in data distribution. Instead of implementing checks and verification algorithms on your own, you can focus on describing how your data should look. Deequ supports you by suggesting checks for you. Deequ is implemented on top of Apache Spark and is designed to scale with large datasets (billions of rows) that typically live in a data lake, distributed file system, or a data warehouse. PyDeequ gives you access to this capability, but also allows you to use it from the familiar environment of your Python [Jupyter](https://jupyter.org/) notebook.\n",
    "\n",
    "## Deequ at Amazon \n",
    "\n",
    "Deequ is used internally at Amazon to verify the quality of many large production datasets. Dataset producers can add and edit data quality constraints. The system computes data quality metrics on a regular basis (with every new version of a dataset), verifies constraints defined by dataset producers, and publishes datasets to consumers in case of success. In error cases, dataset publication can be stopped, and producers are notified to take action. Data quality issues don’t propagate to consumer data pipelines, reducing their area of impact.\n",
    "\n",
    "Deequ is also used within [Amazon SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html#model-monitor-how-it-works). Now with the availability of PyDeequ, you can use it from a broader set of environments — [Amazon SageMaker](https://aws.amazon.com/sagemaker/), [AWS Glue](https://aws.amazon.com/glue/), [Amazon EMR](https://aws.amazon.com/emr/), and more.\n",
    "\n",
    "## Overview of PyDeequ\n",
    "\n",
    "Let’s look at PyDeequ’s main components, and how they relate to Deequ (shown in the following diagram). \n",
    "\n",
    "- __Metrics computation__ – Deequ computes data quality metrics, which are statistics such as completeness, maximum, or correlation. Deequ uses Spark to read from sources such as [Amazon Simple Storage Service](https://aws.amazon.com/s3/) (Amazon S3) and compute metrics through an optimized set of aggregation queries. You have direct access to the raw metrics computed on the data.\n",
    "- __Constraint verification__ – As a user, you focus on defining a set of data quality constraints to be verified. Deequ takes care of deriving the required set of metrics to be computed on the data. Deequ generates a data quality report, which contains the result of the constraint verification.\n",
    "- __Constraint suggestion__ – You can choose to define your own custom data quality constraints or use the automated constraint suggestion methods that profile the data to infer useful constraints.\n",
    "- __Python wrappers__ – You can call each Deequ function using Python syntax. The wrappers translate the commands to the underlying Deequ calls and return their response.\n",
    "\n",
    "![pydeequ-spark-components](../imgs/pydeequ_architecture.jpg)\n",
    "\n",
    "**Figure 1. Overview of PyDeequ components.** \n",
    "\n",
    "## Solution overview \n",
    "\n",
    "As a running example, we have generated a synthetic reviews dataset and introduced various data issues. We demonstrate how to detect these issues using PyDeequ. We begin the way many data science projects do: with initial data exploration and assessment in a Jupyter notebook.\n",
    "\n",
    "During the data exploration phase, we want to answer some basic questions about the data:\n",
    "\n",
    "- Are there fields that have missing values?\n",
    "- How many distinct categories are there in the categorical fields?\n",
    "- Are there correlations between some key features?\n",
    "- If there are two supposedly similar datasets (such as different categories or different time periods), are they really similar?\n",
    "- We also show you how to scale this approach to large-scale datasets, using the same code on an EMR cluster. This is how you’d likely do your ML training as you move into a production setting.\n",
    "\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "In this section we will show how to set up PyDeequ in [SageMaker Notebooks](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html).\n",
    "\n",
    "We use the default VPC for SageMaker Notebooks. The examples presented here use PyDeequ library version 1.2.0 (latest at the time of the update to this notebook) and tested in a SageMaker Notebook instance ml.m5.2xlarge, `conda_python3` kernel.\n",
    " \n",
    "1. Create a new notebook instance. \n",
    "\n",
    "As of version 1.1.0, PyDeequ supports Spark up to version 3.3.0. Your PyDeequ version has to work with your version of Spark.\n",
    "\n",
    "2. In the notebook, run the following lines in a code cell to specify `SPARK_VERSION`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = '3.3' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Install PyDeequ module. For consistency, we'll set the PyDeequ version too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydeequ==1.2.0 in /usr/local/lib/python3.9/dist-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.9/dist-packages (from pydeequ==1.2.0) (2.0.2)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from pydeequ==1.2.0) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23.0->pydeequ==1.2.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23.0->pydeequ==1.2.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23.0->pydeequ==1.2.0) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23.0->pydeequ==1.2.0) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sagemaker_pyspark\n",
      "  Downloading sagemaker_pyspark-1.4.5.tar.gz (181.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sagemaker_pyspark) (2.0.2)\n",
      "Collecting pyspark==3.3.0 (from sagemaker_pyspark)\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5 (from pyspark==3.3.0->sagemaker_pyspark)\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: sagemaker_pyspark, pyspark\n",
      "  Building wheel for sagemaker_pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker_pyspark: filename=sagemaker_pyspark-1.4.5-py3-none-any.whl size=181610611 sha256=6a4aafb71a17028e620b800e74753489e22a0a85994b4457b38fd6228be9c6d7\n",
      "  Stored in directory: /root/.cache/pip/wheels/04/b3/67/fc68544871b31eafddf693b74a2f944ede1ea4c87bb3f8da4b\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764020 sha256=14a118494d82f2867b489cc46ecd1b567915f8036dec1cc2ae619f1a7b48dd3b\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/75/73/81f84d174299abca38dd6a06a5b98b08ae25fce50ab8986fa1\n",
      "Successfully built sagemaker_pyspark pyspark\n",
      "Installing collected packages: py4j, pyspark, sagemaker_pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9\n",
      "    Uninstalling py4j-0.10.9:\n",
      "      Successfully uninstalled py4j-0.10.9\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.0.0\n",
      "    Uninstalling pyspark-3.0.0:\n",
      "      Successfully uninstalled pyspark-3.0.0\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0 sagemaker_pyspark-1.4.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pydeequ==1.2.0\n",
    "!pip install sagemaker_pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To import the modules, run the following commands in a code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker_pyspark\n",
    "import pydeequ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the steps specific to SageMaker Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a PySpark Session\n",
    "\n",
    "In the cell below we import modules and set up a Spark session with the following configurations:\n",
    "\n",
    "- `config(\"spark.driver.extraClassPath\", classpath)` to prepend extra classpath entries to the classpath of the driver\n",
    "- `config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)` to provide Maven of jars to include on the driver and executor classpaths\n",
    "- `config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord` to exclude jars to avoid conflicts\n",
    "- `config(\"spark.driver.memory\", \"15g\")` to increase Java heap space\n",
    "- `config(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")` to read the datetime values as is. In our synthetic dataset we introduced review years and dates such as 1696 to simulate a manual entry error. To ensure that these timestamps are read correctly, this configuration was necessary. See [Spark issue SPARK-31404](https://issues.apache.org/jira/browse/SPARK-31404) about the calendar switch in the version 3.0.\n",
    "\n",
    "For a detailed explanation about these parameters, see [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f27460f4-f4bb-40cf-8730-c9550629aaa3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;2.0.3-spark-3.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "downloading https://repo1.maven.org/maven2/com/amazon/deequ/deequ/2.0.3-spark-3.3/deequ-2.0.3-spark-3.3.jar ...\n",
      "\t[SUCCESSFUL ] com.amazon.deequ#deequ;2.0.3-spark-3.3!deequ.jar (714ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.10!scala-reflect.jar (465ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze_2.12/0.13.2/breeze_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze_2.12;0.13.2!breeze_2.12.jar (824ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze-macros_2.12/0.13.2/breeze-macros_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze-macros_2.12;0.13.2!breeze-macros_2.12.jar (281ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar ...\n",
      "\t[SUCCESSFUL ] com.github.fommil.netlib#core;1.1.2!core.jar (270ms)\n",
      "downloading https://repo1.maven.org/maven2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar ...\n",
      "\t[SUCCESSFUL ] net.sf.opencsv#opencsv;2.3!opencsv.jar (254ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.github.rwl#jtransforms;2.4.0!jtransforms.jar (281ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.2!commons-math3.jar (353ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire_2.12/0.13.0/spire_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire_2.12;0.13.0!spire_2.12.jar (913ms)\n",
      "downloading https://repo1.maven.org/maven2/com/chuusai/shapeless_2.12/2.3.2/shapeless_2.12-2.3.2.jar ...\n",
      "\t[SUCCESSFUL ] com.chuusai#shapeless_2.12;2.3.2!shapeless_2.12.jar(bundle) (380ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.5!slf4j-api.jar (259ms)\n",
      "downloading https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] junit#junit;4.8.2!junit.jar (266ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire-macros_2.12/0.13.0/spire-macros_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire-macros_2.12;0.13.0!spire-macros_2.12.jar (252ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/machinist_2.12/0.6.1/machinist_2.12-0.6.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#machinist_2.12;0.6.1!machinist_2.12.jar (251ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/macro-compat_2.12/1.1.1/macro-compat_2.12-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#macro-compat_2.12;1.1.1!macro-compat_2.12.jar (285ms)\n",
      ":: resolution report :: resolve 11102ms :: artifacts dl 6159ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;2.0.3-spark-3.3 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.1 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   15  |   15  |   2   ||   15  |   15  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f27460f4-f4bb-40cf-8730-c9550629aaa3\n",
      "\tconfs: [default]\n",
      "\t15 artifacts copied, 0 already retrieved (33137kB/65ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/09/19 18:27:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .config(\"spark.driver.memory\", \"15g\")\n",
    "    .config(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.35.22-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore<1.36.0,>=1.35.22 (from boto3)\n",
      "  Downloading botocore-1.35.22-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.36.0,>=1.35.22->boto3) (2.9.0.post0)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore<1.36.0,>=1.35.22->boto3)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.22->boto3) (1.16.0)\n",
      "Downloading boto3-1.35.22-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.35.22-py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "Successfully installed boto3-1.35.22 botocore-1.35.22 jmespath-1.0.1 s3transfer-0.10.2 urllib3-1.26.20\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/titanic.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|  22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|  38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|  26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|  35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|  35|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|  54|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|   2|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|  27|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|  14|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|   4|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|  58|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|  20|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|  39|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|  14|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|  55|    0|    0|          248706|     16| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|   2|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|     13| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|  31|    1|    0|          345763|     18| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you load the DataFrame, you can run `df.printSchema()` to view the schema of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis \n",
    "\n",
    "Before we define checks on the data, we want to calculate some statistics for the dataset. As with Deequ, PyDeequ supports a rich set of metrics. For more information, see [Test data quality at scale with Deequ](https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/) or the [GitHub repo](https://github.com/awslabs/deequ/tree/master/src/main/scala/com/amazon/deequ/analyzers). In the following example, we use the [AnalysisRunner](https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/analyzers/runners/AnalysisRunner.scala) to capture the metrics we’re interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.analyzers import *\n",
    "\n",
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .addAnalyzer(Completeness(\"PassengerId\")) \\\n",
    "                    .addAnalyzer(Distinctness(\"PClass\")) \\\n",
    "                    .addAnalyzer(Mean(\"Fare\")) \\\n",
    "                    .run()\n",
    "                    \n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+--------------------+\n",
      "| entity|   instance|        name|               value|\n",
      "+-------+-----------+------------+--------------------+\n",
      "|Dataset|          *|        Size|               891.0|\n",
      "| Column|PassengerId|Completeness|                 1.0|\n",
      "| Column|     PClass|Distinctness|0.003367003367003367|\n",
      "+-------+-----------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.7g}'.format\n",
    "analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we learn the following:\n",
    "\n",
    "- `review_id` has no missing values and approximately 99.27% of the values are distinct\n",
    "- 74.99% of reviews have a `star_rating` of 4 or higher\n",
    "- `total_votes` and `star_rating` are not correlated\n",
    "- `helpful_votes` and `total_votes` are strongly correlated\n",
    "- The average `star_rating` is 3.99\n",
    "- The dataset contains 3,010,972 reviews\n",
    "\n",
    "Sometimes, you may want to run multiple metrics on a single column. For example, you want to check that all reviews were written either after 1996 or before 2017. In this case, it’s helpful to provide a name for each metric in order to distinguish the results in the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>Fare</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>0.4118967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   entity instance        name     value\n",
       "0  Column     Fare  Compliance 0.4118967"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df) \\\n",
    "                    .addAnalyzer(Compliance(\"Fare\", \n",
    "\"Fare >= 21.000\")) \\\n",
    "                    .run()\n",
    "analysisResult_pd_df = AnalyzerContext.successMetricsAsDataFrame(spark,\n",
    "analysisResult, pandas=True)\n",
    "analysisResult_pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "analysisResult_json = AnalyzerContext.successMetricsAsJson(spark, analysisResult)\n",
    "analysisResult_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Run Tests for Data\n",
    "\n",
    "After analyzing and understanding the data, we want to verify that the properties we have derived also hold for new versions of the dataset. By defining assertions on the data distribution as part of a data pipeline, we can make sure every processed dataset is of high quality, and that any application consuming the data can rely on it.\n",
    "\n",
    "For writing tests on data, we start with the `VerificationSuite` and add [checks](https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/checks/Check.scala) on attributes of the data. In this example, we test for the following properties of our data:\n",
    "\n",
    "- At least 3 million rows in total\n",
    "- `review_id` is never null\n",
    "- `review_id` is unique\n",
    "- `star_rating` has a minimum of 1.0 and maximum of 5.0\n",
    "- `marketplace` only contains `US`, `UK`, `DE`, `JP`, or `FR`\n",
    "- `year` does not contain negative values\n",
    "- `year` is between 1996 and 2017\n",
    "\n",
    "The following code reflects the previous statements. For information about all available checks, see the [GitHub repo](https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/checks/Check.scala). You can run this directly in the Spark shell as previously explained:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Titanic Survival\")\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 800) \\\n",
    "        .hasMin(\"Fare\", lambda x: x > 10000) \\\n",
    "        .isComplete(\"Embarked\")  \\\n",
    "        .isUnique(\"Passenger\")  \\\n",
    "        .isComplete(\"marketplace\")  \\\n",
    "        .isContainedIn(\"sex\", [\"male\", \"female\"]) \\\n",
    "        .isNonNegative(\"age\"))\\\n",
    "    .run()\n",
    "    \n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark,\n",
    "checkResult, pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we change the display settings of the DataFrame to ensure that the entire constraint message is visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Titanic Survival</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>SizeConstraint(Size(None))</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Titanic Survival</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>MinimumConstraint(Minimum(Fare,None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Expected type of column Fare to be one of (Lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Titanic Survival</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>CompletenessConstraint(Completeness(Embarked,N...</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.9977553310886644 does not meet the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Titanic Survival</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(List(Passenger...</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Input data does not include column Passenger!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Titanic Survival</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>CompletenessConstraint(Completeness(marketplac...</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Input data does not include column marketplace!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Titanic Survival</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>ComplianceConstraint(Compliance(sex contained ...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Titanic Survival</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>ComplianceConstraint(Compliance(age is non-neg...</td>\n",
       "      <td>Success</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              check check_level check_status  \\\n",
       "0  Titanic Survival     Warning      Warning   \n",
       "1  Titanic Survival     Warning      Warning   \n",
       "2  Titanic Survival     Warning      Warning   \n",
       "3  Titanic Survival     Warning      Warning   \n",
       "4  Titanic Survival     Warning      Warning   \n",
       "5  Titanic Survival     Warning      Warning   \n",
       "6  Titanic Survival     Warning      Warning   \n",
       "\n",
       "                                          constraint constraint_status  \\\n",
       "0                         SizeConstraint(Size(None))           Success   \n",
       "1              MinimumConstraint(Minimum(Fare,None))           Failure   \n",
       "2  CompletenessConstraint(Completeness(Embarked,N...           Failure   \n",
       "3  UniquenessConstraint(Uniqueness(List(Passenger...           Failure   \n",
       "4  CompletenessConstraint(Completeness(marketplac...           Failure   \n",
       "5  ComplianceConstraint(Compliance(sex contained ...           Success   \n",
       "6  ComplianceConstraint(Compliance(age is non-neg...           Success   \n",
       "\n",
       "                                  constraint_message  \n",
       "0                                                     \n",
       "1  Expected type of column Fare to be one of (Lon...  \n",
       "2  Value: 0.9977553310886644 does not meet the co...  \n",
       "3      Input data does not include column Passenger!  \n",
       "4    Input data does not include column marketplace!  \n",
       "5                                                     \n",
       "6                                                     "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkResult_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_72156_row0_col0, #T_72156_row0_col1, #T_72156_row0_col2, #T_72156_row0_col3, #T_72156_row0_col4, #T_72156_row0_col5, #T_72156_row1_col0, #T_72156_row1_col1, #T_72156_row1_col2, #T_72156_row1_col3, #T_72156_row1_col4, #T_72156_row1_col5, #T_72156_row2_col0, #T_72156_row2_col1, #T_72156_row2_col2, #T_72156_row2_col3, #T_72156_row2_col4, #T_72156_row2_col5, #T_72156_row3_col0, #T_72156_row3_col1, #T_72156_row3_col2, #T_72156_row3_col3, #T_72156_row3_col4, #T_72156_row3_col5, #T_72156_row4_col0, #T_72156_row4_col1, #T_72156_row4_col2, #T_72156_row4_col3, #T_72156_row4_col4, #T_72156_row4_col5, #T_72156_row5_col0, #T_72156_row5_col1, #T_72156_row5_col2, #T_72156_row5_col3, #T_72156_row5_col4, #T_72156_row5_col5, #T_72156_row6_col0, #T_72156_row6_col1, #T_72156_row6_col2, #T_72156_row6_col3, #T_72156_row6_col4, #T_72156_row6_col5 {\n",
       "  overflow-wrap: break-word;\n",
       "  inline-size: 10px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_72156\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_72156_level0_col0\" class=\"col_heading level0 col0\" >check</th>\n",
       "      <th id=\"T_72156_level0_col1\" class=\"col_heading level0 col1\" >check_level</th>\n",
       "      <th id=\"T_72156_level0_col2\" class=\"col_heading level0 col2\" >check_status</th>\n",
       "      <th id=\"T_72156_level0_col3\" class=\"col_heading level0 col3\" >constraint</th>\n",
       "      <th id=\"T_72156_level0_col4\" class=\"col_heading level0 col4\" >constraint_status</th>\n",
       "      <th id=\"T_72156_level0_col5\" class=\"col_heading level0 col5\" >constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_72156_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_72156_row0_col0\" class=\"data row0 col0\" >Titanic Survival</td>\n",
       "      <td id=\"T_72156_row0_col1\" class=\"data row0 col1\" >Warning</td>\n",
       "      <td id=\"T_72156_row0_col2\" class=\"data row0 col2\" >Warning</td>\n",
       "      <td id=\"T_72156_row0_col3\" class=\"data row0 col3\" >SizeConstraint(Size(None))</td>\n",
       "      <td id=\"T_72156_row0_col4\" class=\"data row0 col4\" >Success</td>\n",
       "      <td id=\"T_72156_row0_col5\" class=\"data row0 col5\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72156_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_72156_row1_col0\" class=\"data row1 col0\" >Titanic Survival</td>\n",
       "      <td id=\"T_72156_row1_col1\" class=\"data row1 col1\" >Warning</td>\n",
       "      <td id=\"T_72156_row1_col2\" class=\"data row1 col2\" >Warning</td>\n",
       "      <td id=\"T_72156_row1_col3\" class=\"data row1 col3\" >MinimumConstraint(Minimum(Fare,None))</td>\n",
       "      <td id=\"T_72156_row1_col4\" class=\"data row1 col4\" >Failure</td>\n",
       "      <td id=\"T_72156_row1_col5\" class=\"data row1 col5\" >Expected type of column Fare to be one of (LongType,IntegerType,DoubleType,org.apache.spark.sql.types.DecimalType$@45c276c2,ByteType,FloatType,ShortType), but found StringType instead!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72156_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_72156_row2_col0\" class=\"data row2 col0\" >Titanic Survival</td>\n",
       "      <td id=\"T_72156_row2_col1\" class=\"data row2 col1\" >Warning</td>\n",
       "      <td id=\"T_72156_row2_col2\" class=\"data row2 col2\" >Warning</td>\n",
       "      <td id=\"T_72156_row2_col3\" class=\"data row2 col3\" >CompletenessConstraint(Completeness(Embarked,None))</td>\n",
       "      <td id=\"T_72156_row2_col4\" class=\"data row2 col4\" >Failure</td>\n",
       "      <td id=\"T_72156_row2_col5\" class=\"data row2 col5\" >Value: 0.9977553310886644 does not meet the constraint requirement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72156_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_72156_row3_col0\" class=\"data row3 col0\" >Titanic Survival</td>\n",
       "      <td id=\"T_72156_row3_col1\" class=\"data row3 col1\" >Warning</td>\n",
       "      <td id=\"T_72156_row3_col2\" class=\"data row3 col2\" >Warning</td>\n",
       "      <td id=\"T_72156_row3_col3\" class=\"data row3 col3\" >UniquenessConstraint(Uniqueness(List(Passenger),None))</td>\n",
       "      <td id=\"T_72156_row3_col4\" class=\"data row3 col4\" >Failure</td>\n",
       "      <td id=\"T_72156_row3_col5\" class=\"data row3 col5\" >Input data does not include column Passenger!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72156_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_72156_row4_col0\" class=\"data row4 col0\" >Titanic Survival</td>\n",
       "      <td id=\"T_72156_row4_col1\" class=\"data row4 col1\" >Warning</td>\n",
       "      <td id=\"T_72156_row4_col2\" class=\"data row4 col2\" >Warning</td>\n",
       "      <td id=\"T_72156_row4_col3\" class=\"data row4 col3\" >CompletenessConstraint(Completeness(marketplace,None))</td>\n",
       "      <td id=\"T_72156_row4_col4\" class=\"data row4 col4\" >Failure</td>\n",
       "      <td id=\"T_72156_row4_col5\" class=\"data row4 col5\" >Input data does not include column marketplace!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72156_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_72156_row5_col0\" class=\"data row5 col0\" >Titanic Survival</td>\n",
       "      <td id=\"T_72156_row5_col1\" class=\"data row5 col1\" >Warning</td>\n",
       "      <td id=\"T_72156_row5_col2\" class=\"data row5 col2\" >Warning</td>\n",
       "      <td id=\"T_72156_row5_col3\" class=\"data row5 col3\" >ComplianceConstraint(Compliance(sex contained in male,female,`sex` IS NULL OR `sex` IN ('male','female'),None))</td>\n",
       "      <td id=\"T_72156_row5_col4\" class=\"data row5 col4\" >Success</td>\n",
       "      <td id=\"T_72156_row5_col5\" class=\"data row5 col5\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_72156_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_72156_row6_col0\" class=\"data row6 col0\" >Titanic Survival</td>\n",
       "      <td id=\"T_72156_row6_col1\" class=\"data row6 col1\" >Warning</td>\n",
       "      <td id=\"T_72156_row6_col2\" class=\"data row6 col2\" >Warning</td>\n",
       "      <td id=\"T_72156_row6_col3\" class=\"data row6 col3\" >ComplianceConstraint(Compliance(age is non-negative,COALESCE(CAST(age AS DECIMAL(20,10)), 0.0) >= 0,None))</td>\n",
       "      <td id=\"T_72156_row6_col4\" class=\"data row6 col4\" >Success</td>\n",
       "      <td id=\"T_72156_row6_col5\" class=\"data row6 col5\" ></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fffb5045970>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkResult_df.style.set_properties(\n",
    "    **{\n",
    "        'overflow-wrap': 'break-word',\n",
    "        'inline-size': '10px',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling `run()`, PyDeequ translates your test description into Deequ, which translates it into a series of Spark jobs that are run to compute metrics on the data. Afterwards, it invokes your assertion functions (for example, `lambda x: x == 1.0` for the minimum star rating check) on these metrics to see if the constraints hold on the data. \n",
    "\n",
    "Interestingly, the `review_id` column isn’t unique, which resulted in a failure of the check on uniqueness. We can also look at all the metrics that Deequ computed for this check by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>age is non-negative</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Column</td>\n",
       "      <td>sex contained in male,female</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Column</td>\n",
       "      <td>Embarked</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>0.9977553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity                      instance          name     value\n",
       "0   Column           age is non-negative    Compliance         1\n",
       "1   Column  sex contained in male,female    Compliance         1\n",
       "2  Dataset                             *          Size       891\n",
       "3   Column                      Embarked  Completeness 0.9977553"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkResults_df = VerificationResult.successMetricsAsDataFrame(spark, checkResult, pandas = True)\n",
    "checkResults_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Constraint Suggestion \n",
    "\n",
    "If you own a large number of datasets or if your dataset has many columns, it may be challenging for you to manually define appropriate constraints. Deequ can automatically suggest useful constraints based on the data distribution. Deequ first runs a data profiling method and then applies a set of rules on the result. For more information about how to run a data profiling method, see the [GitHub repo](https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/examples/data_profiling_example.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydeequ.suggestions import *\n",
    "\n",
    "suggestionResult = ConstraintSuggestionRunner(spark) \\\n",
    "             .onData(df) \\\n",
    "             .addConstraintRule(DEFAULT()) \\\n",
    "             .run()\n",
    "\n",
    "# Constraint Suggestions in JSON format\n",
    "print(json.dumps(suggestionResult, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result contains a list of constraints with descriptions and Python code, so that you can directly apply it in your data quality checks. You can call `print(json.dumps(result_json))` to inspect the suggested constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling to Production \n",
    "\n",
    "So far, we’ve shown you how to use these capabilities in the context of data exploration using a Jupyter notebook running on a SageMaker notebook instance. As your project matures, you need to use the same capabilities on larger and larger datasets, and in a production environment. With PyDeequ, it’s straightforward to make that transition. The following diagram illustrates deployment options for local and production purposes on AWS.\n",
    "\n",
    "![pydeequ-in-production](../imgs/pydeequ_deployment.png)\n",
    "\n",
    "**Figure 3. Deployment of PyDeequ in production.** \n",
    "\n",
    "As seen in the diagram above, you can leverage both an AWS EMR cluster and/or AWS Glue for larger or production purposes. To learn more about how to configure an EMR cluster with PyDeequ to explore much larger volumes of data please refer to the AWS blog post [Testing data quality at scale with PyDeequ](https://aws.amazon.com/blogs/big-data/testing-data-quality-at-scale-with-pydeequ/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Examples on GitHub\n",
    "\n",
    "You can find examples of more advanced features on the [Deequ GitHub repo](https://github.com/awslabs/deequ):\n",
    "\n",
    "- Deequ provides more than data quality checks with fixed thresholds. Learn how to use [anomaly detection on data quality metrics](https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/examples/anomaly_detection_example.md) to apply tests on metrics that change over time.\n",
    "- Deequ offers support for storing and loading metrics. Learn how to use the [MetricsRepository](https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/examples/metrics_repository_example.md) for this use case.\n",
    "- If your dataset grows over time or is partitioned, you can use Deequ’s [incremental metrics computation](https://github.com/awslabs/deequ/blob/master/src/main/scala/com/amazon/deequ/examples/algebraic_states_example.md). For each partition, Deequ stores a state for each computed metric. To compute metrics for the union of partitions, Deequ can use these states to efficiently derive overall metrics without reloading the data.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook showed you how to use PyDeequ for calculating data quality metrics, verifying data quality metrics, and profiling data to automate the configuration of data quality checks in an Amazon SageMaker notebook. PyDeequ is available using `pip install` and on GitHub for you to build your own data quality management pipeline.\n",
    "\n",
    "Learn more about the inner workings of Deequ in the VLDB 2018 paper [Automating large-scale data quality verification](https://www.vldb.org/pvldb/vol11/p1781-schelter.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
